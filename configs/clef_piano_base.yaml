# clef-piano-base Training Configuration
# =======================================
#
# ISMIR 2026 target: Swin V2 + ClefAttention for piano transcription
#
# Fair comparison with Zeng et al. (2024):
# - Uses Zeng's vocabulary (extended with grace notes)
# - Same time resolution (100 fps)
# - Same frequency range (A0 = 27.5 Hz)

# =============================================================================
# Seeds
# =============================================================================
seed:
  data_augmentation: 0    # Used during data preprocessing/augmentation
  training: 1234          # Used during model training

# =============================================================================
# Model Architecture
# =============================================================================
model:
  name: "clef-piano-base"

  # Encoder: Swin V2 (frozen)
  swin_model: "microsoft/swinv2-tiny-patch4-window8-256"
  swin_dims: [96, 192, 384, 768]
  freeze_encoder: true

  # Swin stage pooling (remove overlapping-window redundancy)
  # Swin0: RF~320ms, grid 40ms -> pool 8x (Nyquist, chord is discrete)
  # Swin1: RF~640ms, grid 80ms -> pool 4x (50% overlap, melody needs continuity)
  swin_pool_strides: [8, 4, 1, 1]

  # HarmonizingFlow (pitch space transform)
  use_flow: true
  n_harmonics: 6
  flow_pool_stride: 4   # Temporal pooling: T -> T/4 (matches Swin S0, prevents grad explosion)
  use_temporal_cnn: true  # Causal 1D CNN for onset/duration detection
  temporal_pool_stride: 4

  # Deformable Attention (CLEF: Content-aware Learned-prior Event Focusing)
  d_model: 512
  n_heads: 8
  n_levels: 6  # 1 (Flow) + 1 (CNN) + 4 (Swin stages)
  ff_dim: 2048
  dropout: 0.1

  # Square sampling + Content-Dependent Prior (key design)
  n_points_freq: 2          # Frequency direction: local detail
  n_points_time: 2          # Time direction: local detail
  freq_offset_scale: 1.0    # ±1 pixel in feature map space (was 0.15, caused 2x2 degeneration)
  time_offset_scale: 1.0    # ±1 pixel in feature map space (was 0.15, caused 2x2 degeneration)

  # Content-Dependent Reference Points (key innovation)
  use_time_prior: false     # Fixed linspace(0,1,S); learned prior doesn't work because pos_embed std=0.02 is too small
  use_freq_prior: true      # freq_prior(tgt) -> freq region (stream tracking)
  n_freq_groups: 4          # Per-head freq_prior (4 groups x 2 heads = 8 heads)
  refine_range: 0.1         # +/-10% refinement

  # Bridge
  bridge_layers: 2

  # Decoder
  ca_gate_type: "predictive_coding"
  pred_loss_weight: 0.1              # Predictor MSE loss weight (PC mode)
  decoder_layers: 6
  max_seq_len: 16384
  vocab_size: 512           # ~220 factorized tokens + padding

  # Training
  label_smoothing: 0.0

# =============================================================================
# Data
# =============================================================================
data:
  # Datasets
  datasets:
    - musesyn
    - humsyn

  # Vocabulary: Zeng's LabelsMultiple + grace notes
  vocabulary: "zeng_extended"

  # Audio features (matches Zeng's time resolution for fair comparison)
  audio:
    sample_rate: 16000
    feature: "log_mel"      # Zeng uses VQT, but Mel enables Swin V2 pretrained weights
    n_mels: 128             # Zeng: 480 bins (60 x 8 octaves)
    n_fft: 2048
    hop_length: 160         # 100 fps (same as Zeng)
    f_min: 27.5             # A0 = 27.5 Hz (piano lowest, same as Zeng)
    f_max: 7040.0           # 8 octaves from A0

  # Chunking for long pieces
  chunking:
    enabled: true
    chunk_frames: 24000     # 4 min @ 100 fps
    overlap_frames: 12000   # 2 min overlap
    min_chunk_ratio: 0.5    # Last chunk at least 2 min

  # Augmentation (matching Zeng)
  augmentation:
    transpose:
      enabled: true
    soundfonts:
      - "TimGM6mb.sf2"
      - "FluidR3_GM.sf2"
      - "UprightPianoKW-20220221.sf2"
      - "SalamanderGrandPiano-V3+20200602.sf2"
    loudness_norm:
      target_lufs: -15

# =============================================================================
# Training
# =============================================================================
training:
  distributed:
    enabled: true
    backend: "nccl"
    num_gpus: 2

  precision: "bf16"
  batch_size: 1             # per GPU
  gradient_accumulation_steps: 2
  effective_batch_size: 4   # 2 GPU x 1 batch x 2 accum

  learning_rate: 2.0e-5      # Base LR; offset params use 0.1x (2e-6); gamma uses constant 2e-4
  weight_decay: 0.01
  max_epochs: 50
  warmup_steps: 5000        # ~5.3 epochs warmup (944 steps/epoch)
  gradient_clip: 1.0

  save_every_n_epochs: 10
  save_best: true
  save_last: true
  early_stopping_patience: 5

  wandb:
    enabled: true
    project: "clef-piano-base"
    tags: ["piano", "a2s", "swin", "clef-attention", "multi-head", "rtx3090", "harmonic-flow", "predictive-coding"]

# =============================================================================
# Paths
# =============================================================================
paths:
  data_dir: "data/datasets"
  output_dir: "data/experiments/clef_piano_base"
  checkpoint_dir: "checkpoints/clef_piano_base"
